# LLM Provider

ç»Ÿä¸€çš„ LLM æä¾›å•†æ¥å£ï¼Œä½¿ç”¨ Vercel AI SDK å®ç°ã€‚æ”¯æŒ DeepSeekã€OpenAI å’Œ Google (Gemini) å¤šä¸ªæä¾›å•†ã€‚

## ç‰¹æ€§

- ğŸ”Œ ç»Ÿä¸€çš„æ¥å£è®¾è®¡ï¼Œæ”¯æŒå¤šä¸ª LLM æä¾›å•†
- ğŸ”„ å®Œæ•´çš„æµå¼å’Œéæµå¼å“åº”æ”¯æŒ
- ğŸ› ï¸ å†…ç½®å·¥å…·è°ƒç”¨ï¼ˆFunction Callingï¼‰æ”¯æŒ
- ğŸ’¬ è‡ªåŠ¨ç®¡ç†å¯¹è¯å†å²
- ğŸ§  æ”¯æŒæ€è€ƒæ¨¡å¼ï¼ˆDeepSeek Reasonerï¼‰
- ğŸ“Š Token ä½¿ç”¨ç»Ÿè®¡
- ğŸ¯ ç±»å‹å®‰å…¨çš„ TypeScript å®ç°

## æ”¯æŒçš„æä¾›å•†

- **DeepSeek**: deepseek-chat, deepseek-reasoner
- **OpenAI**: gpt-4o, gpt-4o-mini, o1-preview, ç­‰
- **Google**: gemini-2.0-flash-exp, gemini-1.5-pro, ç­‰

## å®‰è£…ä¾èµ–

```bash
yarn add @ai-sdk/deepseek @ai-sdk/openai @ai-sdk/google ai
```

## ç¯å¢ƒå˜é‡é…ç½®

```env
# DeepSeek
DEEPSEEK_API_KEY=your_deepseek_api_key
DEEPSEEK_BASE_URL=https://api.deepseek.com
DEEPSEEK_API_MODEL=deepseek-chat
DEEPSEEK_API_REASON_MODEL=deepseek-reasoner

# OpenAI
OPENAI_API_KEY=your_openai_api_key
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_API_MODEL=gpt-4o-mini
OPENAI_ORGANIZATION=your_org_id

# Google (Gemini)
GEMINI_API_KEY=your_gemini_api_key
GEMINI_BASE_URL=https://generativelanguage.googleapis.com/v1beta
GEMINI_API_MODEL=gemini-2.0-flash-exp
```

## ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬èŠå¤©

```typescript
import { ProviderFactory, ProviderType } from '@/lib/llm-provider';

// åˆ›å»ºæä¾›å•†å®ä¾‹
const provider = ProviderFactory.getProvider(ProviderType.DeepSeek);

// å‘é€æ¶ˆæ¯
const response = await provider.chat({
  dialogId: 'user-123',
  input: 'ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±',
});

console.log(response.content);
console.log(response.metadata);
```

### æµå¼å“åº”

```typescript
const response = await provider.chatStream({
  dialogId: 'user-123',
  input: 'å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½çš„æ–‡ç« ',
  onChunk: (chunk) => {
    process.stdout.write(chunk);
  },
});
```

### ä½¿ç”¨å·¥å…·è°ƒç”¨

```typescript
const tools = [
  {
    name: 'get_weather',
    description: 'è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯',
    parameters: {
      type: 'object',
      properties: {
        city: {
          type: 'string',
          description: 'åŸå¸‚åç§°',
        },
      },
      required: ['city'],
    },
    handler: async ({ city }) => {
      // å®ç°è·å–å¤©æ°”çš„é€»è¾‘
      return { city, temp: 25, condition: 'æ™´æœ—' };
    },
  },
];

const response = await provider.chatWithTools({
  dialogId: 'user-123',
  input: 'åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ',
  tools,
});
```

### DeepSeek æ€è€ƒæ¨¡å¼

```typescript
const deepseekProvider = ProviderFactory.getProvider(ProviderType.DeepSeek);

const response = await deepseekProvider.chat({
  dialogId: 'user-123',
  input: 'è§£é‡Šé‡å­çº ç¼ çš„åŸç†',
  isReason: true, // å¯ç”¨æ€è€ƒæ¨¡å¼
});

// æŸ¥çœ‹æ¨ç†è¿‡ç¨‹
console.log('æ¨ç†è¿‡ç¨‹:', response.metadata.reasoningContent);
console.log('æœ€ç»ˆç­”æ¡ˆ:', response.content);
```

### è‡ªåŠ¨é€‰æ‹©æä¾›å•†

```typescript
import { ProviderFactory } from '@/lib/llm-provider';

// æ ¹æ®æ¨¡å‹åç§°è‡ªåŠ¨é€‰æ‹©æä¾›å•†
const provider = ProviderFactory.getProviderByModel('gpt-4o');
const response = await provider.chat({
  dialogId: 'user-123',
  input: 'Hello!',
});
```

### ä½¿ç”¨é»˜è®¤æä¾›å•†

```typescript
import { getDefaultProvider } from '@/lib/llm-provider';

// è‡ªåŠ¨æ ¹æ®ç¯å¢ƒå˜é‡é€‰æ‹©å¯ç”¨çš„æä¾›å•†
const provider = getDefaultProvider();
const response = await provider.chat({
  dialogId: 'user-123',
  input: 'ä½ å¥½',
});
```

### è‡ªå®šä¹‰é…ç½®

```typescript
import { ProviderFactory } from '@/lib/llm-provider';

const provider = ProviderFactory.createDeepSeek({
  apiKey: 'custom_api_key',
  model: 'deepseek-chat',
  temperature: 0.8,
  maxTokens: 4000,
  systemPrompt: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»å­¦åŠ©æ‰‹',
});
```

### ç®¡ç†å¯¹è¯å†å²

```typescript
// è·å–å†å²è®°å½•
const history = provider.getHistory('user-123');
console.log(history);

// æ¸…é™¤å†å²è®°å½•
provider.clearHistory('user-123');

// è®¾ç½®ç³»ç»Ÿæç¤ºè¯
provider.setSystemPrompt('user-123', 'ä½ æ˜¯ä¸€ä¸ªå‹å–„çš„åŠ©æ‰‹');
```

## API æ–‡æ¡£

### Provider æ¥å£

æ‰€æœ‰æä¾›å•†éƒ½å®ç°äº†ç»Ÿä¸€çš„ `Provider` æ¥å£ï¼š

```typescript
interface Provider {
  readonly type: ProviderType;
  readonly model: string;
  readonly reasonModel?: string;
  
  chat(options: ChatOptions): Promise<ChatResponse>;
  chatStream(options: ChatOptions): Promise<ChatResponse>;
  chatWithTools(options: ChatOptions): Promise<ChatResponse>;
  chatWithToolsStream(options: ChatOptions): Promise<ChatResponse>;
  
  clearHistory(dialogId: string): void;
  getHistory(dialogId: string): Message[];
  setSystemPrompt(dialogId: string, prompt: string): void;
}
```

### ChatOptions

```typescript
interface ChatOptions {
  dialogId: string;        // å¯¹è¯ ID
  input: string;           // ç”¨æˆ·è¾“å…¥
  isReason?: boolean;      // æ˜¯å¦ä½¿ç”¨æ€è€ƒæ¨¡å¼ï¼ˆä»… DeepSeekï¼‰
  tools?: Tool[];          // å·¥å…·åˆ—è¡¨
  onChunk?: ChunkHandler;  // æµå¼å“åº”å¤„ç†å™¨
}
```

### ChatResponse

```typescript
interface ChatResponse {
  content: string;                 // å“åº”å†…å®¹
  metadata: ResponseMetadata;      // å“åº”å…ƒæ•°æ®
}

interface ResponseMetadata {
  model: string;                   // ä½¿ç”¨çš„æ¨¡å‹
  provider: ProviderType;          // æä¾›å•†ç±»å‹
  timestamp: string;               // æ—¶é—´æˆ³
  isReason?: boolean;              // æ˜¯å¦ä¸ºæ€è€ƒæ¨¡å¼
  reasoningContent?: string;       // æ¨ç†è¿‡ç¨‹ï¼ˆä»… DeepSeek Reasonerï¼‰
  toolCalls?: ToolCall[];          // å·¥å…·è°ƒç”¨ä¿¡æ¯
  usage?: UsageInfo;               // Token ä½¿ç”¨ç»Ÿè®¡
}
```

## æ¶æ„è®¾è®¡

```
llm-provider/
â”œâ”€â”€ index.ts           # ä¸»å…¥å£ï¼Œæä¾›å•†å·¥å‚
â”œâ”€â”€ types.ts           # ç±»å‹å®šä¹‰
â”œâ”€â”€ config.ts          # é…ç½®ç®¡ç†
â”œâ”€â”€ history.ts         # å¯¹è¯å†å²ç®¡ç†
â”œâ”€â”€ utils.ts           # å·¥å…·å‡½æ•°
â”œâ”€â”€ providers/
â”‚   â”œâ”€â”€ deepseek.ts    # DeepSeek å®ç°
â”‚   â”œâ”€â”€ openai.ts      # OpenAI å®ç°
â”‚   â””â”€â”€ google.ts      # Google å®ç°
â””â”€â”€ README.md          # æ–‡æ¡£
```

## è¿ç§»æŒ‡å—

### ä»æ—§çš„ DeepSeek å®¢æˆ·ç«¯è¿ç§»

```typescript
// æ—§ä»£ç 
import { chatClient } from '@/lib/deepseek';
const response = await chatClient.chatStream(dialogId, input, onChunk);

// æ–°ä»£ç 
import { ProviderFactory, ProviderType } from '@/lib/llm-provider';
const provider = ProviderFactory.getProvider(ProviderType.DeepSeek);
const response = await provider.chatStream({ dialogId, input, onChunk });
```

### ä»æ—§çš„ OpenAI å®¢æˆ·ç«¯è¿ç§»

```typescript
// æ—§ä»£ç 
import { chatClient } from '@/lib/openai';
const response = await chatClient.chat(dialogId, input);

// æ–°ä»£ç 
import { ProviderFactory, ProviderType } from '@/lib/llm-provider';
const provider = ProviderFactory.getProvider(ProviderType.OpenAI);
const response = await provider.chat({ dialogId, input });
```

## æœ€ä½³å®è·µ

1. **ä½¿ç”¨å•ä¾‹æ¨¡å¼**: é€šè¿‡ `ProviderFactory.getProvider()` è·å–æä¾›å•†å®ä¾‹ï¼Œé¿å…é‡å¤åˆ›å»º
2. **åˆç†ä½¿ç”¨ dialogId**: æ¯ä¸ªç”¨æˆ·ä¼šè¯ä½¿ç”¨å”¯ä¸€çš„ dialogId æ¥ç®¡ç†å¯¹è¯å†å²
3. **é”™è¯¯å¤„ç†**: æ‰€æœ‰æ–¹æ³•éƒ½å¯èƒ½æŠ›å‡ºå¼‚å¸¸ï¼Œå»ºè®®ä½¿ç”¨ try-catch åŒ…è£¹
4. **æµå¼å“åº”**: å¯¹äºé•¿æ–‡æœ¬ç”Ÿæˆï¼Œä¼˜å…ˆä½¿ç”¨æµå¼ API æå‡ç”¨æˆ·ä½“éªŒ
5. **å·¥å…·è°ƒç”¨**: åˆ©ç”¨å·¥å…·è°ƒç”¨åŠŸèƒ½å®ç°å¤æ‚çš„äº¤äº’é€»è¾‘

## è®¸å¯è¯

MIT

