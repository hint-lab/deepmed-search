/**
 * Answer Action Handler
 * 
 * å¤„ç†ç”Ÿæˆç­”æ¡ˆçš„åŠ¨ä½œï¼ŒåŒ…æ‹¬ï¼š
 * 1. å¯¹ç­”æ¡ˆè¿›è¡Œå¤šç»´åº¦è¯„ä¼°ï¼ˆdefinitive, freshness, plurality, completeness, strictï¼‰
 * 2. åŒºåˆ†ä¸»é—®é¢˜å’Œå­é—®é¢˜çš„å¤„ç†é€»è¾‘
 * 3. è¯„ä¼°å¤±è´¥æ—¶è¿›è¡Œé”™è¯¯åˆ†æå’Œåæ€
 * 4. ç®¡ç†è¯„ä¼°é‡è¯•æ¬¡æ•°å’Œæ”¹è¿›è®¡åˆ’
 * 
 * @param thisAgent - ç ”ç©¶ä»£ç†å®ä¾‹
 * @param action - ç­”æ¡ˆåŠ¨ä½œå¯¹è±¡ï¼ŒåŒ…å«ç”Ÿæˆçš„ç­”æ¡ˆå†…å®¹
 * @param currentQuestion - å½“å‰æ­£åœ¨å›ç­”çš„é—®é¢˜
 * @returns Promise<boolean> - è¿”å› true è¡¨ç¤ºåº”è¯¥ä¸­æ–­ä¸»å¾ªç¯ï¼ˆæ‰¾åˆ°æœ€ç»ˆç­”æ¡ˆæˆ–æ— é‡è¯•æ¬¡æ•°ï¼‰
 */

import {
    AnswerAction, EvaluationResponse, TrackerContext, KnowledgeItem
} from '../types';
import { evaluateAnswer } from '../tools/evaluator';
import { analyzeSteps } from '../tools/error-analyzer';
import { Schemas } from "../utils/schemas";
import { ResearchAgent } from '../agent';
import { updateContextHelper } from '../agent-helpers';
import { formatDateBasedOnType } from "../utils/date-tools";
import { publishThink } from '../tracker-store';


export async function handleAnswerAction(thisAgent: ResearchAgent, action: AnswerAction, currentQuestion: string): Promise<boolean> {
    console.log("Handling Answer Action for:", currentQuestion);

    const totalStep = thisAgent.totalStep as number;
    const options = thisAgent.options;
    const evaluationMetrics = thisAgent.evaluationMetrics as Record<string, any[]>;
    const context = thisAgent.context as TrackerContext;
    const SchemaGen = thisAgent.SchemaGen as Schemas;
    const allKnowledge = thisAgent.allKnowledge as KnowledgeItem[];
    const question = thisAgent.question as string;
    const diaryContext = thisAgent.diaryContext as string[];
    const finalAnswerPIP = thisAgent.finalAnswerPIP as string[];
    const gaps = thisAgent.gaps as string[];
    await publishThink(thisAgent.context.taskId, `æ­¥éª¤ ${totalStep}: å¼€å§‹å›ç­”`);
    
    // ========== 1. æ£€æŸ¥æ˜¯å¦ä¸ºç®€å•é—®é¢˜ï¼ˆç¬¬ä¸€æ­¥å¯ç›´æ¥å›ç­”ï¼‰==========
    if (totalStep === 1 && !options.noDirectAnswer) {
        console.log("Trivial question or direct answer allowed on first step.");
        action.isFinal = true;
        (thisAgent as any).trivialQuestion = true; // Modify agent state
        updateContextHelper(thisAgent, { totalStep: totalStep, question: currentQuestion, ...action });
        return true; // Break the loop
    }

    // ========== 2. è¯„ä¼°ç­”æ¡ˆè´¨é‡ ==========
    // åˆå§‹åŒ–è¯„ä¼°ç»“æœï¼Œé»˜è®¤é€šè¿‡
    let evaluation: EvaluationResponse = {
        pass: true,
        think: 'Evaluation skipped or passed by default.',
        type: 'strict',
        improvement_plan: ''
    };
    const currentEvalMetrics = evaluationMetrics[currentQuestion];

    // å¦‚æœæœ‰è¯„ä¼°æŒ‡æ ‡ï¼Œåˆ™è¿›è¡Œè¯„ä¼°
    if (currentEvalMetrics && currentEvalMetrics.length > 0) {
        console.log(`Evaluating answer for: ${currentQuestion} with metrics:`, currentEvalMetrics.map(e => e.type));

        context.actionTracker.trackThink('eval_first', SchemaGen.languageCode);
        try {
            evaluation = await evaluateAnswer(
                currentQuestion,
                action,
                currentEvalMetrics.filter(e => e.numEvalsRequired > 0).map(e => e.type),
                context,
                allKnowledge,
                SchemaGen
            ) || evaluation;
        } catch (evalError) {
            console.error(`Error during answer evaluation for ${currentQuestion}:`, evalError);
            evaluation = { pass: false, think: `Evaluation failed with error: ${evalError instanceof Error ? evalError.message : String(evalError)}`, type: 'strict' };
        }
    } else {
        console.log(`No evaluation metrics found for: ${currentQuestion}, skipping evaluation.`);
    }

    // ========== 3. å¤„ç†è¯„ä¼°ç»“æœ ==========
    if (currentQuestion.trim() === question) {
        // ---------- 3.1 å¤„ç†ä¸»é—®é¢˜çš„è¯„ä¼°ç»“æœ ----------
        if (evaluation.pass) {
            // ä¸»é—®é¢˜ç­”æ¡ˆé€šè¿‡è¯„ä¼°ï¼Œä»»åŠ¡å®Œæˆ
            diaryContext.push(`
At step ${thisAgent.step}, you took **answer** action and finally found the answer to the original question:
Original question: ${currentQuestion}
Your answer: ${action.answer}
The evaluator thinks your answer is good because: ${evaluation.think}
Your journey ends here. Congratulations! ğŸ‰
`);
            action.isFinal = true;
            updateContextHelper(thisAgent, { totalStep: totalStep, question: currentQuestion, ...action });
            return true; // ä¸­æ–­å¾ªç¯ - æ‰¾åˆ°æœ€ç»ˆç­”æ¡ˆ
        } else {
            // ä¸»é—®é¢˜ç­”æ¡ˆæœªé€šè¿‡è¯„ä¼°ï¼Œéœ€è¦æ”¹è¿›
            diaryContext.push(`
At step ${thisAgent.step}, you took **answer** action but evaluator thinks it is not a good answer:
Original question: ${currentQuestion}
Your answer: ${action.answer}
The evaluator thinks your answer is bad because: ${evaluation.think}
`);
            await publishThink(thisAgent.context.taskId, `æ­¥éª¤ ${totalStep}: ä¸»é—®é¢˜ç­”æ¡ˆè¯„ä¼°å¤±è´¥`);
            
            // å‡å°‘å¤±è´¥ç±»å‹çš„è¯„ä¼°é‡è¯•æ¬¡æ•°
            if (currentEvalMetrics) {
                evaluationMetrics[currentQuestion] = currentEvalMetrics.map(e => {
                    if (e.type === evaluation.type) {
                        e.numEvalsRequired--;
                    }
                    return e;
                }).filter(e => e.numEvalsRequired > 0);
            }

            // å¦‚æœæ˜¯ä¸¥æ ¼è¯„ä¼°ä¸”æœ‰æ”¹è¿›è®¡åˆ’ï¼Œæ·»åŠ åˆ°æ”¹è¿›è®¡åˆ’åˆ—è¡¨
            if (evaluation.type === 'strict' && evaluation.improvement_plan) {
                finalAnswerPIP.push(evaluation.improvement_plan);
            }

            // æ£€æŸ¥æ˜¯å¦è¿˜æœ‰é‡è¯•æœºä¼š
            if (!evaluationMetrics[currentQuestion] || evaluationMetrics[currentQuestion].length === 0) {
                console.warn(`No more evaluation attempts left for the main question: ${currentQuestion}. Returning current answer.`);
                action.isFinal = false;
                (thisAgent as any).thisStep = action;
                updateContextHelper(thisAgent, { totalStep: totalStep, question: currentQuestion, ...action });
                return true; // ä¸­æ–­å¾ªç¯ - æ— æ›´å¤šé‡è¯•æ¬¡æ•°
            }

            // åˆ†ææ­¥éª¤å¹¶æ·»åŠ åæ€åˆ°çŸ¥è¯†åº“
            try {
                const errorAnalysis = await analyzeSteps(diaryContext, context, SchemaGen);
                allKnowledge.push({
                    question: `Why is the following answer bad for the question? Please reflect\n<question>${currentQuestion}</question>\n<answer>${action.answer}</answer>`,
                    answer: `${evaluation.think}\n\n${errorAnalysis.recap}\n\n${errorAnalysis.blame}\n\n${errorAnalysis.improvement}`,
                    type: 'qa',
                });
            } catch (analysisError) {
                console.error("Error during step analysis after failed evaluation:", analysisError);
                // å¦‚æœåˆ†æå¤±è´¥ï¼Œæ·»åŠ ç®€å•çš„åæ€
                allKnowledge.push({
                    question: `Reflection on why the answer failed evaluation for question: ${currentQuestion}`,
                    answer: `The answer was evaluated as needing improvement for reason: ${evaluation.think}. Step analysis failed.`,
                    type: 'qa'
                });
            }
            
            // é‡ç½®æ—¥å¿—å’Œæ­¥éª¤è®¡æ•°å™¨ï¼Œå‡†å¤‡ä¸‹ä¸€è½®åæ€
            (thisAgent as any).diaryContext = [];
            (thisAgent as any).step = 0;
            (thisAgent as any).allowAnswer = false;
            updateContextHelper(thisAgent, { totalStep: totalStep, question: currentQuestion, ...action, evaluation });
        }
    } else {
        // ---------- 3.2 å¤„ç†å­é—®é¢˜çš„è¯„ä¼°ç»“æœ ----------
        if (evaluation.pass) {
            // å­é—®é¢˜ç­”æ¡ˆé€šè¿‡è¯„ä¼°ï¼Œæ·»åŠ åˆ°çŸ¥è¯†åº“
            diaryContext.push(`
                At step ${thisAgent.step}, you took **answer** action. You found a good answer to the sub-question:
                Sub-question: ${currentQuestion}
                Your answer: ${action.answer}
                The evaluator thinks your answer is good because: ${evaluation.think}
                Adding this to knowledge.`);
            allKnowledge.push({
                question: currentQuestion,
                answer: action.answer,
                type: 'qa',
                updated: formatDateBasedOnType(new Date(), 'full')
            });

            // ä»å¾…è§£å†³é—®é¢˜åˆ—è¡¨ä¸­ç§»é™¤å·²å›ç­”çš„å­é—®é¢˜
            const gapIndex = gaps.indexOf(currentQuestion);
            if (gapIndex > -1) {
                gaps.splice(gapIndex, 1);
            }
            updateContextHelper(thisAgent, { totalStep: totalStep, question: currentQuestion, ...action, evaluation });
        } else {
            // å­é—®é¢˜ç­”æ¡ˆæœªé€šè¿‡è¯„ä¼°ï¼Œä¸æ·»åŠ åˆ°çŸ¥è¯†åº“
            diaryContext.push(`
                At step ${thisAgent.step}, you took **answer** action for the sub-question: ${currentQuestion}.
                Your answer: ${action.answer}
                However, the evaluator thinks your answer is bad because: ${evaluation.think}
                This answer will not be added to the knowledge base.`);
            updateContextHelper(thisAgent, { totalStep: totalStep, question: currentQuestion, ...action, evaluation });
        }
    }

    return false; // ç»§ç»­å¾ªç¯ï¼Œé™¤éä¸Šé¢æ˜ç¡®ä¸­æ–­
}
