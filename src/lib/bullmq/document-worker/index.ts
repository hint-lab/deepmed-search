import { createWorker, addTask } from '../queue-manager';
import { DocumentProcessJobData, DocumentProcessJobResult } from './types';
import { parseDocument } from '../../document-parser';
import { TaskType } from '../types';
import { Job } from 'bullmq';
import logger from '@/utils/logger';
import { getReadableUrl, uploadFileStream, getFileUrl } from '../../minio/operations';
import { userDocumentContextStorage, UserDocumentContext } from '../../document-parser/user-context';
import { decryptApiKey } from '@/lib/crypto';
import { PrismaClient } from '@prisma/client';
import { IDocumentProcessingStatus } from '@/types/enums';
import {
    updateDocumentProgress,
    updateDocumentStatus,
    reportDocumentError,
    reportDocumentComplete
} from '@/lib/document-tracker';
import { ChunkIndexJobData } from '../chunk-worker/types';
import { normalizeLanguage } from '@/constants/language';
import { Readable } from 'stream';

const prisma = new PrismaClient();

// æ–‡æ¡£å¤„ç†å‡½æ•°
export async function processDocument(data: DocumentProcessJobData): Promise<DocumentProcessJobResult> {
    const { documentId, userId, options, documentInfo } = data;

    try {
        if (!documentInfo || !documentInfo.uploadFile) {
            throw new Error('æ–‡æ¡£ä¿¡æ¯ä¸å®Œæ•´');
        }

        // ä¿®å¤æ–‡ä»¶è·¯å¾„ï¼Œç§»é™¤é‡å¤çš„ deepmed ç›®å½•
        const filePath = documentInfo.uploadFile.location.replace(/^deepmed\//, '');
        logger.info('å¤„ç†æ–‡ä»¶è·¯å¾„', {
            documentId,
            originalPath: documentInfo.uploadFile.location,
            fixedPath: filePath
        });

        // ç”Ÿæˆå¯è¯»çš„ MinIO URL
        const fileUrl = await getReadableUrl('deepmed', filePath);
        logger.info('ç”Ÿæˆæ–‡ä»¶ URL', {
            documentId,
            fileUrl
        });

        // ä½¿ç”¨ç»Ÿä¸€çš„æ–‡æ¡£è§£æå™¨ï¼ˆä»ç”¨æˆ·é…ç½®ä¸­è·å–ï¼‰
        const result = await parseDocument(fileUrl, {
            fileName: documentInfo.name || filePath.split('/').pop() || 'document',
            maintainFormat: options.maintainFormat,
            prompt: options.prompt || '',
            documentId: documentId, // ä¼ é€’ documentId ç”¨äºå›¾ç‰‡ä¸Šä¼ 
            language: options.language,
        });

        // è½¬æ¢ DocumentParseResult åˆ° DocumentProcessJobResult æ ¼å¼
        return {
            success: result.success,
            data: result.success ? {
                pages: result.pages?.map(page => ({
                    content: page.content,
                    contentLength: page.content.length,
                })),
                extracted: result.content || '',
                summary: result.pages ? {
                    totalPages: result.pages.length,
                    ocr: {
                        successful: result.pages.length,
                        failed: 0,
                    },
                    extracted: result.content || '',
                } : undefined,
            } : undefined,
            error: result.error || '',
            metadata: {
                ...result.metadata,
                fileUrl,
                language: options.language,
            },
        };
    } catch (error) {
        logger.error('æ–‡æ¡£å¤„ç†å¤±è´¥', {
            documentId,
            error: error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯',
            stack: error instanceof Error ? error.stack : undefined
        });
        throw error;
    }
}

// åˆ›å»ºæ–‡æ¡£å¤„ç†worker
export const documentWorker = createWorker<DocumentProcessJobData, DocumentProcessJobResult>(
    TaskType.DOCUMENT_CONVERT_TO_MD,
    async (job: Job<DocumentProcessJobData, DocumentProcessJobResult>) => {
        const { documentId, userId } = job.data;

        try {
            // æ¨é€è¿›åº¦ï¼šå¼€å§‹åŠ è½½é…ç½®
            await updateDocumentProgress(documentId, 5, 'åŠ è½½ç”¨æˆ·é…ç½®...');

            // ä»æ•°æ®åº“åŠ è½½ç”¨æˆ·çš„æ–‡æ¡£è§£æå™¨é…ç½®ï¼ˆåªæŸ¥è¯¢ä¸€æ¬¡ï¼‰
            logger.info(`[Document Worker] Loading user config for user ${userId}, document ${documentId}`);

            const userConfig = await prisma.searchConfig.findUnique({
                where: { userId },
            });

            if (!userConfig) {
                const errorMsg = 'æœªæ‰¾åˆ°ç”¨æˆ·æœç´¢é…ç½®ã€‚è¯·è®¿é—® /settings/document é¡µé¢é…ç½®æ–‡æ¡£è§£æå™¨';
                await reportDocumentError(documentId, errorMsg);
                throw new Error(errorMsg);
            }

            // æ„å»ºç”¨æˆ·æ–‡æ¡£å¤„ç†ä¸Šä¸‹æ–‡
            const documentContext: UserDocumentContext = {
                userId,
                documentParser: userConfig.documentParser as any,
                mineruApiKey: userConfig.mineruApiKey ? decryptApiKey(userConfig.mineruApiKey) : undefined,
            };

            logger.info(`[Document Worker] ğŸ“„ User ${userId.substring(0, 8)}... using parser: ${documentContext.documentParser}`);

            // æ¨é€è¿›åº¦ï¼šå¼€å§‹å¤„ç†æ–‡æ¡£
            await updateDocumentProgress(documentId, 10, 'å¼€å§‹å¤„ç†æ–‡æ¡£...');
            await updateDocumentStatus(documentId, IDocumentProcessingStatus.CONVERTING, 'æ­£åœ¨è½¬æ¢æ–‡æ¡£');
            // æ³¨æ„ï¼šCONVERTING çŠ¶æ€ä¸å†™å…¥æ•°æ®åº“ï¼Œåªé€šè¿‡ SSE æ¨é€

            // ä½¿ç”¨ AsyncLocalStorage åœ¨éš”ç¦»çš„ä¸Šä¸‹æ–‡ä¸­è¿è¡Œæ–‡æ¡£å¤„ç†ä»»åŠ¡
            const result = await userDocumentContextStorage.run(documentContext, async () => {
                // æ›´æ–°è¿›åº¦ï¼šå¼€å§‹å¤„ç†
                await job.updateProgress(10);

                // æ¨é€è¿›åº¦ï¼šæ­£åœ¨è§£æ
                await updateDocumentProgress(documentId, 30, 'æ­£åœ¨è§£ææ–‡æ¡£å†…å®¹...');

                const processResult = await processDocument(job.data);

                // æ¨é€è¿›åº¦ï¼šè§£æå®Œæˆ
                await updateDocumentProgress(documentId, 40, 'æ–‡æ¡£è§£æå®Œæˆ');

                // æ›´æ–°è¿›åº¦ï¼šè½¬æ¢é˜¶æ®µå®Œæˆ
                await job.updateProgress(50);

                return processResult;
            });

            // è½¬æ¢å®Œæˆï¼Œä¿å­˜è½¬æ¢ç»“æœåˆ°æ•°æ®åº“
            if (result.success && result.data) {
                const doc = await prisma.document.findUnique({
                    where: { id: documentId },
                    select: {
                        process_begin_at: true,
                        knowledgeBaseId: true,
                        name: true
                    }
                });

                if (!doc || !doc.knowledgeBaseId) {
                    throw new Error(`æ–‡æ¡£ ${documentId} ç¼ºå°‘çŸ¥è¯†åº“ID`);
                }

                // è·å–çŸ¥è¯†åº“é…ç½®
                const knowledgeBase = await prisma.knowledgeBase.findUnique({
                    where: { id: doc.knowledgeBaseId },
                    select: {
                        chunk_size: true,
                        overlap_size: true,
                        split_by: true,
                        language: true,
                    }
                });

                const jobLanguage = job.data.options.language || knowledgeBase?.language || undefined;
                const normalizedLanguage = normalizeLanguage(jobLanguage);

                const startTime = doc?.process_begin_at?.getTime() || Date.now();
                const duration = Math.floor((Date.now() - startTime) / 1000);

                // å°† markdown å†…å®¹ä¸Šä¼ åˆ° MinIO
                let markdownUrl: string | null = null;
                const markdownContent = result.data.extracted || '';

                // æ£€æŸ¥ markdown å†…å®¹æ˜¯å¦ä¸ºç©º
                if (!markdownContent || markdownContent.trim() === '') {
                    const errorMsg = `æ–‡æ¡£ ${documentId} è½¬æ¢åçš„ markdown å†…å®¹ä¸ºç©ºï¼Œæ— æ³•ç»§ç»­å¤„ç†`;
                    logger.error(`[Document Worker] ${errorMsg}`);
                    await reportDocumentError(documentId, errorMsg);
                    throw new Error(errorMsg);
                }

                try {
                    const buffer = Buffer.from(markdownContent, 'utf8');
                    const stream = new Readable();
                    stream.push(buffer);
                    stream.push(null);

                    const objectName = `documents/${documentId}/markdown.md`;
                    await uploadFileStream({
                        bucketName: 'deepmed',
                        objectName,
                        stream,
                        size: buffer.length,
                        metaData: {
                            'content-type': 'text/markdown; charset=utf-8'
                        }
                    });

                    markdownUrl = await getFileUrl('deepmed', objectName);
                    logger.info(`[Document Worker] Markdown å†…å®¹å·²ä¸Šä¼ è‡³ MinIO: ${markdownUrl}`, {
                        documentId,
                        markdownUrl,
                        contentLength: markdownContent.length
                    });
                } catch (error) {
                    const errorMsg = `ä¸Šä¼  Markdown å†…å®¹åˆ° MinIO å¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`;
                    logger.error(`[Document Worker] ${errorMsg}`, { documentId, error });
                    await reportDocumentError(documentId, errorMsg);
                    throw error;
                }

                // ç¡®ä¿ markdown_url å·²è®¾ç½®
                if (!markdownUrl) {
                    const errorMsg = `æ–‡æ¡£ ${documentId} çš„ markdown_url æœªè®¾ç½®ï¼Œæ— æ³•ç»§ç»­å¤„ç†`;
                    logger.error(`[Document Worker] ${errorMsg}`);
                    await reportDocumentError(documentId, errorMsg);
                    throw new Error(errorMsg);
                }

                // ä¿å­˜è½¬æ¢ç»“æœï¼ˆmarkdown URL å­˜å‚¨åœ¨ markdown_urlï¼‰
                // å¦‚æœ file_url è¿˜æ²¡æœ‰è®¾ç½®ï¼Œåˆ™ä» metadata ä¸­è·å–å¹¶ä¿å­˜
                const updateData: any = {
                    markdown_url: markdownUrl, // å­˜å‚¨ markdown çš„ URLï¼ˆå¿…é¡»è®¾ç½®ï¼‰
                    processing_status: IDocumentProcessingStatus.CONVERTED, // è½¬æ¢å®Œæˆï¼Œå¯ä»¥å¼€å§‹ç´¢å¼•
                    progress: 50,
                    progress_msg: 'è½¬æ¢å®Œæˆï¼Œç­‰å¾…åˆ†å—ç´¢å¼•',
                    process_duation: duration,
                };

                // å¦‚æœ file_url ä¸ºç©ºä¸” metadata ä¸­æœ‰ fileUrlï¼Œåˆ™è®¾ç½®å®ƒï¼ˆå¤„ç†æ—§æ•°æ®ï¼‰
                if (result.metadata?.fileUrl) {
                    const currentDoc = await prisma.document.findUnique({
                        where: { id: documentId },
                        select: { file_url: true }
                    });
                    if (!currentDoc?.file_url) {
                        updateData.file_url = result.metadata.fileUrl;
                        logger.info(`[Document Worker] æ–‡æ¡£ ${documentId} è¡¥å……è®¾ç½® file_url: ${result.metadata.fileUrl}`);
                    }
                }

                await prisma.document.update({
                    where: { id: documentId },
                    data: updateData
                });

                await updateDocumentStatus(documentId, IDocumentProcessingStatus.CONVERTED, 'è½¬æ¢å®Œæˆï¼Œç­‰å¾…åˆ†å—ç´¢å¼•');
                await updateDocumentProgress(documentId, 50, 'è½¬æ¢å®Œæˆï¼Œç­‰å¾…åˆ†å—ç´¢å¼•', {
                    converted: true,
                    language: normalizedLanguage,
                });
                logger.info(`[Document Worker] æ–‡æ¡£ ${documentId} è½¬æ¢å®Œæˆï¼Œå·²ä¿å­˜å†…å®¹ï¼Œå‡†å¤‡æ·»åŠ åˆ°åˆ†å—ç´¢å¼•é˜Ÿåˆ—`);

                // å°†åˆ†å—ç´¢å¼•ä»»åŠ¡æ·»åŠ åˆ°é˜Ÿåˆ—
                const chunkIndexJobData: ChunkIndexJobData = {
                    documentId,
                    kbId: doc.knowledgeBaseId,
                    userId,
                    options: {
                        model: job.data.options.model,
                        maintainFormat: job.data.options.maintainFormat,
                        prompt: job.data.options.prompt,
                        documentName: doc.name,
                        maxChunkSize: knowledgeBase?.chunk_size || 2000,
                        overlapSize: knowledgeBase?.overlap_size || 200,
                        splitByParagraph: knowledgeBase?.split_by === 'paragraph' || knowledgeBase?.split_by === 'page',
                        language: jobLanguage,
                    }
                };

                const chunkJobId = await addTask(
                    TaskType.CHUNK_VECTOR_INDEX,
                    chunkIndexJobData,
                    `chunk-index-${documentId}`
                );

                logger.info(`[Document Worker] æ–‡æ¡£ ${documentId} çš„åˆ†å—ç´¢å¼•ä»»åŠ¡å·²æ·»åŠ åˆ°é˜Ÿåˆ— (Job ID: ${chunkJobId})`);

                // æ¨é€è½¬æ¢å®ŒæˆçŠ¶æ€ï¼ˆä½¿ç”¨ progress metadata è¡¨ç¤ºï¼‰
                await updateDocumentProgress(documentId, 50, 'è½¬æ¢å®Œæˆï¼Œç­‰å¾…åˆ†å—ç´¢å¼•', {
                    converted: true,
                    chunkJobId,
                    language: normalizedLanguage,
                });
            } else {
                // è½¬æ¢å¤±è´¥
                await prisma.document.update({
                    where: { id: documentId },
                    data: {
                        processing_status: IDocumentProcessingStatus.FAILED,
                        progress: 0,
                        progress_msg: result.error || 'è½¬æ¢å¤±è´¥',
                    }
                });
            }

            return result;
        } catch (error) {
            logger.error(`[Document Worker] Document ${documentId} processing failed:`, error);

            // æ¨é€é”™è¯¯çŠ¶æ€åˆ° Redis
            const errorMsg = error instanceof Error ? error.message : 'æ–‡æ¡£å¤„ç†å¤±è´¥';
            await reportDocumentError(documentId, errorMsg);

            // æ›´æ–°è¿›åº¦ï¼šå¤„ç†å¤±è´¥
            await job.updateProgress(-1);
            throw error;
        }
    }
); 